CSCI 3320 Fundamentals of Machine Learning
Course Project
Steve  - 11550, Atif Khurshid 1155085457
2. The Dataset and pre-processing
2.2 Data pre-processing
2.2.3 Indices and features for horses, jockeys and trainers
Number of horses:  2155
Number of jockeys:  105
Number of trainers:  93

RBF Kernel:
This kernel can handle the case when the relation between class
labels and attributes is nonlinear. Furthermore, since the linear kernel is a special case 
of RBF kernel, it can also handle linear relationship. Moreover, RBF and sigmoid can have the
same results for certain parameters and sigmoid can even be invalid for certain parameters.
It also has hyperparameters than polynomial kernel which influence the complexity
of model selection. Finally, the number of features is small and RBF performs well for small
number of features.
Epsilon is the width of SVM's soft boundary whereas C is the penalty for violating this 
soft boundary. Hence, a larger C and smaller epsilon result in a complex model which may overfit
whereas smaller parameters mean that the model is less complex. We've selected epsilon = 0.1 and
C = 5000 because these parameters result in the highest top1 accuracy and lowest RMSE.
 





SVR: svr_model = SVR(kernel='rbf', C = 5000, epsilon=0.1, gamma= 0.000001)
     (svr_model, 1.567, 0.200, 0.473, 4.763)
GBRT: gbrt_model = GradientBoostingRegressor(loss='ls', learning_rate=0.05, n_estimators=100, max_depth=8, random_state=42)
     (gbrt_model, 1.568, 0.229, 0.490, 4.510)

SVR: Prev = 1.567	Norm = 1.591
	RMSE has increased but top1 accuracy has also increased
GBRT: Prev = 1.568	Norm = 1.567
	RMSE has decreased slightly but top1 result is unaffected

model_name          RMSE   Top_1   Top_3     Average_Rank
svr_model,         1.567   0.200    0.473       4.763
Scaled svr_model,  1.591   0.223    0.494       4.515
gbrt_model,        1.568   0.229    0.490       4.510
Scaled gbrt_model, 1.567   0.229    0.492       4.502

